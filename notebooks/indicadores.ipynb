{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h1 align=\"center\"><strong>Cálculo de dependencia cruzada para todos los países</strong></h1>\n",
    "<h4 align=\"center\"><strong>Manuel Alejandro Hidalgo y Jorge Díaz Lanchas</strong></h4>\n",
    "<h4 align=\"center\"><strong>Fundación Real Instituto Elcano</strong></h4>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***No ejecutar este código a menos que se quiera comprimir***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprimir_dividir_archivo(archivo_original, tamano_maximo=100, directorio_salida=None):\n",
    "    # Asegúrate de que el archivo original existe\n",
    "    archivo_original = Path(archivo_original)\n",
    "    if not archivo_original.exists():\n",
    "        raise FileNotFoundError(f\"No se encuentra el archivo: {archivo_original}\")\n",
    "    \n",
    "    # Si no se especifica directorio de salida, usar src/data/raw/ITP/\n",
    "    if directorio_salida is None:\n",
    "        # Obtener el directorio raíz del proyecto (donde está src/)\n",
    "        proyecto_root = Path(__file__).parent.parent.parent\n",
    "        directorio_salida = proyecto_root / 'src' / 'data' / 'raw' / 'ITP'\n",
    "    else:\n",
    "        directorio_salida = Path(directorio_salida)\n",
    "    \n",
    "    # Crear el directorio de salida si no existe\n",
    "    directorio_salida.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Abre el archivo original en modo de lectura binaria\n",
    "    with open(archivo_original, 'rb') as f_in:\n",
    "        # Lee el contenido del archivo original\n",
    "        contenido = f_in.read()\n",
    "        \n",
    "        # Determina el número de partes necesarias\n",
    "        num_partes = (len(contenido) + tamano_maximo - 1) // tamano_maximo\n",
    "        \n",
    "        # Divide el contenido en partes y escribe cada parte comprimida\n",
    "        for i in range(num_partes):\n",
    "            parte = contenido[i * tamano_maximo: (i + 1) * tamano_maximo]\n",
    "            archivo_salida = directorio_salida / f'ITPD_E_R02.csv.parte{i}.gz'\n",
    "            with gzip.open(archivo_salida, 'wb') as f_out:\n",
    "                f_out.write(parte)\n",
    "            print(f\"Parte {i} creada en: {archivo_salida}\")\n",
    "\n",
    "# Tamaño máximo por parte (1GB)\n",
    "tamano_maximo = 1000 * 1024 * 1024\n",
    "\n",
    "try:\n",
    "    # Ruta al archivo original\n",
    "    archivo_original = Path(r'Datos/ITP/ITPD_E_R02.csv')\n",
    "    \n",
    "    # Comprimir y dividir el archivo original\n",
    "    comprimir_dividir_archivo(archivo_original, tamano_maximo)\n",
    "    print(\"Proceso completado con éxito\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el proceso: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Descomprimir, carga de datos y borrado de archivo***\n",
    "\n",
    "La compresión se hace para poder trabajar con git sin porblemas de tamaño de ficheros.\n",
    "Se descomprime, se importa y luego se borra el fichero descomprimido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directorio fuente: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\raw\\ITP\n",
      "Directorio destino: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\n",
      "Combinando archivos comprimidos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando archivos: 100%|██████████| 7/7 [00:54<00:00,  7.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo archivo CSV...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtrando datos de 2019: 726it [04:36,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo temporal eliminado\n",
      "Total de países únicos encontrados: 237\n",
      "Procesamiento completado con éxito\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FASE 1: PREPARACIÓN Y CARGA DE DATOS\n",
    "Este script procesa la base de datos International Trade and Production Database (ITP)\n",
    "que viene dividida en múltiples archivos comprimidos\n",
    "\"\"\"\n",
    "def procesar_datos_itp():\n",
    "    try:\n",
    "        # Definición de rutas usando Path y la estructura de tu proyecto\n",
    "        # Si estamos en un notebook, usamos una ruta relativa\n",
    "        try:\n",
    "            base_path = Path(__file__).parent.parent.parent\n",
    "        except NameError:  # Estamos en un notebook\n",
    "            base_path = Path.cwd().parent  # Asumiendo que el notebook está en /notebooks/\n",
    "\n",
    "        source_directory = base_path / \"src\" / \"data\" / \"raw\" / \"ITP\"\n",
    "        target_directory = base_path / \"src\" / \"data\" / \"processed\"\n",
    "        target_filename = 'ITPD_E_R02.csv'\n",
    "\n",
    "        # Imprimir las rutas para verificación\n",
    "        print(f\"Directorio fuente: {source_directory}\")\n",
    "        print(f\"Directorio destino: {target_directory}\")\n",
    "\n",
    "        # Asegurar que los directorios existen\n",
    "        target_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Verificar que el directorio fuente existe\n",
    "        if not source_directory.exists():\n",
    "            raise FileNotFoundError(f\"No se encuentra el directorio fuente: {source_directory}\")\n",
    "\n",
    "        # Listar archivos comprimidos\n",
    "        chunk_filenames = sorted([f for f in os.listdir(source_directory) \n",
    "                         if f.startswith('ITPD_E_R02.csv.parte') and f.endswith('.gz')])\n",
    "\n",
    "        # Control de errores: verificar que existen archivos para procesar\n",
    "        if not chunk_filenames:\n",
    "            raise FileNotFoundError(f\"No se encontraron archivos .gz en {source_directory}\")\n",
    "\n",
    "        # Construir la ruta completa para el archivo combinado\n",
    "        target_filepath = target_directory / target_filename\n",
    "\n",
    "        print(\"Combinando archivos comprimidos...\")\n",
    "        with open(target_filepath, 'wb') as target_file:\n",
    "            for chunk_filename in tqdm(chunk_filenames, desc=\"Procesando archivos\"):\n",
    "                chunk_filepath = source_directory / chunk_filename\n",
    "                with gzip.open(chunk_filepath, 'rb') as chunk_file:\n",
    "                    target_file.write(chunk_file.read())\n",
    "\n",
    "        print(\"Leyendo archivo CSV...\")\n",
    "        # Usar chunks para manejar archivos grandes de manera eficiente\n",
    "        itp = pd.read_csv(target_filepath, sep=\",\", chunksize=100000)\n",
    "        \n",
    "        # Procesar por chunks y filtrar año 2019\n",
    "        chunks_2019 = []\n",
    "        for chunk in tqdm(itp, desc=\"Filtrando datos de 2019\"):\n",
    "            chunk_2019 = chunk[chunk['year'] == 2019]\n",
    "            chunks_2019.append(chunk_2019)\n",
    "        \n",
    "        # Combinar todos los chunks filtrados\n",
    "        itp2019 = pd.concat(chunks_2019, ignore_index=True)\n",
    "\n",
    "        # Limpieza: eliminar archivo temporal\n",
    "        os.remove(target_filepath)\n",
    "        print(f\"Archivo temporal eliminado\")\n",
    "\n",
    "        # Obtener lista única de países importadores\n",
    "        codigos_countries = list(itp2019['importer_iso3'].unique())\n",
    "        print(f\"Total de países únicos encontrados: {len(codigos_countries)}\")\n",
    "\n",
    "        return itp2019, codigos_countries\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante el procesamiento: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data, countries = procesar_datos_itp()\n",
    "        print(\"Procesamiento completado con éxito\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error en la ejecución principal: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase AnalisisDependenciaComercial - Documentación Detallada\n",
    "\n",
    "## Descripción General\n",
    "Esta clase implementa un análisis completo de dependencia comercial entre países, utilizando matrices de flujos comerciales para calcular índices de dependencia directa e indirecta.\n",
    "\n",
    "## Estructura de la Clase\n",
    "\n",
    "### Constructor\n",
    "```python\n",
    "def __init__(self, codigos_paises: List[str])\n",
    "```\n",
    "- **Propósito**: Inicializa la clase con la lista de países a analizar\n",
    "- **Atributos**:\n",
    "  - `self.codigos_paises`: Lista ordenada de códigos ISO3 de países\n",
    "  - `self.matrices`: Diccionario para almacenar matrices de comercio por industria\n",
    "  - `self.matrices_O`: Almacena las submatrices Omega para cálculos intermedios\n",
    "\n",
    "### Método: crear_matriz_comercio\n",
    "```python\n",
    "def crear_matriz_comercio(self, grouped_data) -> Dict[str, pd.DataFrame]\n",
    "```\n",
    "- **Propósito**: Crea matrices de flujos comerciales para cada industria\n",
    "- **Funcionamiento**:\n",
    "  1. Valida que los datos tengan las columnas necesarias\n",
    "  2. Para cada industria:\n",
    "     - Crea una matriz base de ceros\n",
    "     - Filtra transacciones válidas\n",
    "     - Asigna valores de comercio a la matriz\n",
    "- **Parámetros**:\n",
    "  - `grouped_data`: DataFrame agrupado por industria\n",
    "- **Retorna**: Diccionario de matrices de comercio por industria\n",
    "\n",
    "### Método: mover_fila_columna\n",
    "```python\n",
    "@staticmethod\n",
    "def mover_fila_columna(df: pd.DataFrame, nombre: str) -> pd.DataFrame\n",
    "```\n",
    "- **Propósito**: Reordena una matriz moviendo un país específico al final\n",
    "- **Importancia**: Necesario para el cálculo de vectores de dependencia\n",
    "- **Proceso**:\n",
    "  1. Valida existencia del país en filas y columnas\n",
    "  2. Crea nueva lista de índices con el país al final\n",
    "  3. Reordena el DataFrame\n",
    "\n",
    "### Método: calcular_vectores_ae\n",
    "```python\n",
    "def calcular_vectores_ae(self, pais: str) -> Tuple[Dict, Dict]\n",
    "```\n",
    "- **Propósito**: Calcula vectores de dependencia directa\n",
    "- **Proceso**:\n",
    "  1. Valida existencia de matrices y país\n",
    "  2. Para cada industria:\n",
    "     - Normaliza la matriz por columnas\n",
    "     - Mueve el país analizado al final\n",
    "     - Establece diagonal en ceros\n",
    "     - Extrae vector de dependencia\n",
    "- **Retorna**: Tupla con vectores de dependencia y matrices normalizadas\n",
    "\n",
    "### Método: calcular_dependencia_total\n",
    "```python\n",
    "def calcular_dependencia_total(self, pais: str) -> pd.DataFrame\n",
    "```\n",
    "- **Propósito**: Calcula la dependencia total (directa + indirecta)\n",
    "- **Proceso**:\n",
    "  1. Obtiene vectores de dependencia directa\n",
    "  2. Construye matrices Omega (submatrices)\n",
    "  3. Calcula matrices inversas (I - Ω)^(-1)\n",
    "  4. Multiplica vectores por matrices inversas\n",
    "- **Matemática Subyacente**:\n",
    "  - Usa la fórmula d = ae(I - Ω)^(-1)\n",
    "  - Captura efectos directos e indirectos\n",
    "\n",
    "### Método: get_summary_stats\n",
    "```python\n",
    "def get_summary_stats(self, dependencia: pd.DataFrame) -> pd.DataFrame\n",
    "```\n",
    "- **Propósito**: Genera estadísticas descriptivas de la dependencia\n",
    "- **Estadísticas calculadas**:\n",
    "  - Media\n",
    "  - Mediana\n",
    "  - Desviación estándar\n",
    "  - Máximo\n",
    "  - Mínimo\n",
    "\n",
    "## Consideraciones Técnicas\n",
    "1. **Manejo de Errores**:\n",
    "   - Validación de datos de entrada\n",
    "   - Control de divisiones por cero\n",
    "   - Manejo de errores en inversión de matrices\n",
    "\n",
    "2. **Eficiencia**:\n",
    "   - Uso de NumPy para operaciones matriciales\n",
    "   - Manejo eficiente de memoria con copy()\n",
    "   - Vectorización donde es posible\n",
    "\n",
    "3. **Precisión Numérica**:\n",
    "   - Manejo de valores NaN\n",
    "   - Normalización de matrices\n",
    "   - Control de errores numéricos\n",
    "\n",
    "## Ejemplo de Uso\n",
    "```python\n",
    "# Inicializar análisis\n",
    "analisis = AnalisisDependenciaComercial(codigos_paises)\n",
    "\n",
    "# Crear matrices de comercio\n",
    "analisis.crear_matriz_comercio(grouped_data)\n",
    "\n",
    "# Calcular dependencia para un país\n",
    "dependencia = analisis.calcular_dependencia_total('ESP')\n",
    "\n",
    "# Obtener estadísticas\n",
    "stats = analisis.get_summary_stats(dependencia)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from typing import Dict, List, Tuple\n",
    "import warnings\n",
    "\n",
    "\n",
    "class AnalisisDependenciaComercial:\n",
    "    def __init__(self, codigos_paises: List[str]):\n",
    "        self.codigos_paises = sorted(codigos_paises)  # Lista ordenada de códigos de países\n",
    "        self.matrices = {}                            # Diccionario para almacenar matrices\n",
    "        self.matrices_O = None                        # Para matrices O (presumiblemente matrices origen)\n",
    "        self.resultados_dependencia = {}              # Para almacenar resultados del análisis\n",
    "        \n",
    "        \n",
    "    def crear_matriz_comercio(self, grouped_data) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Crea matrices de comercio bilateral para cada industria a partir de datos agrupados.\n",
    "        \n",
    "        Esta función procesa datos de comercio internacional y crea una matriz para cada industria\n",
    "        donde las filas representan países exportadores y las columnas países importadores.\n",
    "        Los valores en la matriz representan el volumen de comercio entre cada par de países.\n",
    "        \n",
    "        Args:\n",
    "            grouped_data: DataFrame agrupado por industria que contiene columnas:\n",
    "                        - exporter_iso3: código ISO3 del país exportador\n",
    "                        - importer_iso3: código ISO3 del país importador\n",
    "                        - trade: valor del comercio bilateral\n",
    "                        \n",
    "        Returns:\n",
    "            Dict[str, pd.DataFrame]: Diccionario donde:\n",
    "                                    - keys: nombres de industrias\n",
    "                                    - values: matrices de comercio bilateral\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: Si faltan columnas requeridas en los datos\n",
    "        \"\"\"\n",
    "        # Inicializar diccionario vacío para almacenar las matrices de cada industria\n",
    "        matrices = {}\n",
    "        \n",
    "        # Definir conjunto de columnas que deben estar presentes en los datos\n",
    "        required_columns = {'exporter_iso3', 'importer_iso3', 'trade'}\n",
    "        \n",
    "        # Verificar si todas las columnas requeridas están presentes en los datos\n",
    "        # grouped_data.obj accede al DataFrame original antes del groupby\n",
    "        if not required_columns.issubset(grouped_data.obj.columns):\n",
    "            raise ValueError(f\"Los datos deben contener las columnas: {required_columns}\")\n",
    "        \n",
    "        # Iterar sobre cada industria y su grupo de datos correspondiente\n",
    "        # tqdm añade una barra de progreso para monitorear el avance\n",
    "        for industry, group in tqdm(grouped_data, desc=\"Creando matrices de comercio\"):\n",
    "            # Crear una matriz vacía inicializada con ceros\n",
    "            # Tanto filas como columnas son los códigos de países definidos en self.codigos_paises\n",
    "            matrix_df = pd.DataFrame(\n",
    "                0.0,  # Valor inicial para todas las celdas\n",
    "                index=self.codigos_paises,    # Países exportadores en las filas\n",
    "                columns=self.codigos_paises   # Países importadores en las columnas\n",
    "            )\n",
    "            \n",
    "            # Filtrar solo las transacciones donde tanto exportador como importador\n",
    "            # están en la lista de países definida (self.codigos_paises)\n",
    "            valid_trades = group[\n",
    "                group['exporter_iso3'].isin(self.codigos_paises) & \n",
    "                group['importer_iso3'].isin(self.codigos_paises)\n",
    "            ]\n",
    "            \n",
    "            # Para cada transacción válida, asignar el valor de comercio\n",
    "            # a la posición correspondiente en la matriz\n",
    "            for _, row in valid_trades.iterrows():\n",
    "                # .at es más eficiente que .loc para acceder a valores individuales\n",
    "                matrix_df.at[row['exporter_iso3'], row['importer_iso3']] = row['trade']\n",
    "            \n",
    "            # Almacenar la matriz completada en el diccionario, usando el nombre\n",
    "            # de la industria como clave\n",
    "            matrices[industry] = matrix_df\n",
    "        \n",
    "        # Guardar las matrices en el atributo de la clase para uso posterior\n",
    "        self.matrices = matrices\n",
    "        \n",
    "        # Devolver el diccionario de matrices\n",
    "        return matrices\n",
    "        \n",
    "    @staticmethod\n",
    "    def mover_fila_columna(df: pd.DataFrame, nombre: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Mueve una fila y columna específica al final de la matriz.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame que queremos reordenar\n",
    "            nombre: Nombre de la fila/columna que queremos mover al final\n",
    "        \"\"\"\n",
    "        # Comprobar si el nombre existe en filas y columnas\n",
    "        if nombre not in df.index or nombre not in df.columns:\n",
    "            raise ValueError(f\"'{nombre}' debe estar presente en filas y columnas\")\n",
    "        \n",
    "        # Crear copia para no modificar el DataFrame original        \n",
    "        df = df.copy()\n",
    "        \n",
    "        # Crear lista nueva de columnas: primero todas menos 'nombre', y 'nombre' al final\n",
    "        cols = [col for col in df.columns if col != nombre] + [nombre]\n",
    "        \n",
    "        # Crear lista nueva de filas: primero todas menos 'nombre', y 'nombre' al final\n",
    "        rows = [idx for idx in df.index if idx != nombre] + [nombre]\n",
    "        \n",
    "        # Reordenar el DataFrame con las nuevas listas de filas y columnas\n",
    "        return df.reindex(columns=cols, index=rows)\n",
    "\n",
    "    def calcular_vectores_ae(self, pais: str) -> Tuple[Dict, Dict]:\n",
    "        if not self.matrices:\n",
    "            raise ValueError(\"Debe llamar a crear_matriz_comercio primero\")\n",
    "        if pais not in self.codigos_paises:\n",
    "            raise ValueError(f\"País '{pais}' no encontrado en los códigos de países\")\n",
    "            \n",
    "        vectores_ae = {}\n",
    "        matrices_normalizadas = {}\n",
    "        \n",
    "        for industry, matrix in self.matrices.items():\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                \n",
    "                column_sum = matrix.sum(axis=0)\n",
    "                column_sum = column_sum.replace(0, np.nan)\n",
    "                normalized_matrix = matrix.div(column_sum, axis=1)\n",
    "                normalized_matrix = normalized_matrix.fillna(0)\n",
    "                \n",
    "                normalized_matrix = self.mover_fila_columna(normalized_matrix, pais)\n",
    "                \n",
    "                np.fill_diagonal(normalized_matrix.values, 0)\n",
    "                \n",
    "                matrices_normalizadas[industry] = normalized_matrix\n",
    "                vectores_ae[industry] = normalized_matrix.iloc[-1][:-1]\n",
    "        \n",
    "        return vectores_ae, matrices_normalizadas\n",
    "\n",
    "    def calcular_dependencia_total(self, pais: str) -> tuple[dict, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Calcula la dependencia total (directa + indirecta) de un país respecto a otros países\n",
    "        para cada industria.\n",
    "\n",
    "        Args:\n",
    "            pais (str): Código del país para el que se calcula la dependencia\n",
    "\n",
    "        Returns:\n",
    "            tuple: (matrices_inversas, depend) donde:\n",
    "                matrices_inversas (dict): Diccionario de matrices inversas de Leontief por industria\n",
    "                depend (pd.DataFrame): Matriz de dependencias totales\n",
    "        \"\"\"\n",
    "        # Obtener vectores de dependencia directa y matrices normalizadas\n",
    "        vectores_ae, matrices_normalizadas = self.calcular_vectores_ae(pais)\n",
    "        \n",
    "        # Crear diccionario de matrices O (matrices sin el país analizado)\n",
    "        matrices_O = {}\n",
    "        for industry, matrix in matrices_normalizadas.items():\n",
    "            if matrix is not None and not matrix.empty:\n",
    "                if matrix.shape[0] > 1 and matrix.shape[1] > 1:\n",
    "                    matrices_O[industry] = matrix.iloc[:-1, :-1]\n",
    "        \n",
    "        # Verificar si hay matrices válidas para procesar\n",
    "        if not matrices_O:\n",
    "            raise ValueError(\"No hay matrices válidas para procesar\")\n",
    "        \n",
    "        # Guardar matrices_O como atributo de la clase\n",
    "        self.matrices_O = matrices_O\n",
    "        \n",
    "        # Calcular las matrices inversas de Leontief\n",
    "        matrices_inversas = {}\n",
    "        for industry, matrix in matrices_O.items():\n",
    "            matrix_values = matrix.fillna(0).values\n",
    "            try:\n",
    "                inverse = inv(np.eye(matrix_values.shape[0]) - matrix_values)\n",
    "                matrices_inversas[industry] = inverse\n",
    "            except np.linalg.LinAlgError:\n",
    "                warnings.warn(f\"No se pudo calcular la inversa para {industry}\")\n",
    "                continue\n",
    "        \n",
    "        # Calcular dependencia total\n",
    "        dependencia = {}\n",
    "        for industry, inverse_matrix in matrices_inversas.items():\n",
    "            ae = vectores_ae[industry]\n",
    "            resultado = np.dot(ae.fillna(0), inverse_matrix)\n",
    "            dependencia[industry] = resultado\n",
    "        \n",
    "        # Crear DataFrame de dependencias\n",
    "        depend = pd.DataFrame(dependencia).T\n",
    "        depend.columns = next(iter(matrices_O.values())).columns\n",
    "        \n",
    "        # Asegurarse de devolver exactamente dos valores\n",
    "        return matrices_O, matrices_inversas, depend\n",
    "\n",
    "    def get_summary_stats(self, dependencia: pd.DataFrame) -> pd.DataFrame:\n",
    "        return pd.DataFrame({\n",
    "            'Media': dependencia.mean(),\n",
    "            'Mediana': dependencia.median(),\n",
    "            'Desv. Est.': dependencia.std(),\n",
    "            'Máximo': dependencia.max(),\n",
    "            'Mínimo': dependencia.min()\n",
    "        })\n",
    "    def calcular_dependencias_todos_paises(self, datos_comercio: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Calcula las dependencias comerciales para todos los países en el conjunto de datos.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        datos_comercio : DataFrame\n",
    "            DataFrame con los datos de comercio bilateral (debe contener las columnas\n",
    "            'exporter_iso3', 'importer_iso3', 'industry_descr', 'trade')\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        dict\n",
    "            Diccionario donde las claves son los países y los valores son DataFrames\n",
    "            con sus dependencias comerciales por industria\n",
    "        \"\"\"\n",
    "\n",
    "        # Crear el directorio si no existe para guardar las matrices de dependencia\n",
    "        output_dir = Path.cwd().parent / \"src\" / \"data\" / \"processed\" / \"Matrices de Dependencia\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "        # Validar columnas necesarias\n",
    "        required_columns = {'exporter_iso3', 'importer_iso3', 'industry_descr', 'trade'}\n",
    "        if not required_columns.issubset(datos_comercio.columns):\n",
    "            raise ValueError(f\"Los datos deben contener las columnas: {required_columns}\")\n",
    "        \n",
    "        # Agrupar los datos por industria para el análisis\n",
    "        grouped_data = datos_comercio.groupby('industry_descr')\n",
    "        \n",
    "        # Crear las matrices de comercio iniciales\n",
    "        print(\"Creando matrices de comercio iniciales...\")\n",
    "        self.crear_matriz_comercio(grouped_data)\n",
    "        \n",
    "        # Procesar cada país\n",
    "        print(\"Calculando dependencias para cada país...\")\n",
    "        for pais in tqdm(self.codigos_paises, desc=\"Procesando países\"):\n",
    "            try:\n",
    "                # Calcular la dependencia total para el país actual\n",
    "                dependencia = self.calcular_dependencia_total(pais)\n",
    "                \n",
    "                # Almacenar resultados para este país\n",
    "                self.resultados_dependencia[pais] = dependencia\n",
    "\n",
    "                # Guardar la matriz en un archivo\n",
    "                output_path = os.path.join(output_dir, f\"matriz_dependencia_de_{pais}.csv\")\n",
    "                dependencia[2].to_csv(output_path, sep=\";\") # dependencia[2] porque es una tupla y queremos el DataFrame\n",
    "        \n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando país {pais}: {str(e)}\")\n",
    "        \n",
    "        return self.resultados_dependencia\n",
    "    \n",
    "    def analizar_resultados(self) -> Dict[str, Dict[str, pd.DataFrame]]:\n",
    "        \"\"\"\n",
    "        Analiza y resume los resultados de dependencia para todos los países.\n",
    "        \n",
    "        Retorna:\n",
    "        --------\n",
    "        dict\n",
    "            Diccionario con diferentes análisis de los resultados, incluyendo:\n",
    "            - Estadísticas por país\n",
    "            - Rankings de dependencia\n",
    "            - Análisis de concentración\n",
    "        \"\"\"\n",
    "        if not self.resultados_dependencia:\n",
    "            raise ValueError(\"No hay resultados para analizar. Ejecute calcular_dependencias_todos_paises primero.\")\n",
    "        \n",
    "        analisis = {}\n",
    "        \n",
    "        # 1. Estadísticas básicas por país\n",
    "        stats_por_pais = {}\n",
    "        for pais, depend in self.resultados_dependencia.items():\n",
    "            stats = pd.DataFrame({\n",
    "                'Media': depend.mean(),\n",
    "                'Mediana': depend.median(),\n",
    "                'Max': depend.max(),\n",
    "                'Min': depend.min(),\n",
    "                'Std': depend.std()\n",
    "            })\n",
    "            stats_por_pais[pais] = stats\n",
    "        analisis['estadisticas_por_pais'] = stats_por_pais\n",
    "        \n",
    "        # 2. Rankings de dependencia por industria\n",
    "        rankings = {}\n",
    "        for pais, depend in self.resultados_dependencia.items():\n",
    "            # Ordenar industrias por nivel de dependencia media\n",
    "            ranking = depend.mean(axis=1).sort_values(ascending=False)\n",
    "            rankings[pais] = ranking\n",
    "        analisis['rankings'] = rankings\n",
    "        \n",
    "        # 3. Análisis de concentración (Herfindahl)\n",
    "        concentracion = {}\n",
    "        for pais, depend in self.resultados_dependencia.items():\n",
    "            # Calcular índice de Herfindahl por industria\n",
    "            herfindahl = (depend ** 2).sum(axis=1)\n",
    "            concentracion[pais] = herfindahl\n",
    "        analisis['concentracion'] = concentracion\n",
    "        \n",
    "        return analisis\n",
    "    \n",
    "    def obtener_resumen_pais(self, pais: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Genera un resumen detallado para un país específico.\n",
    "        \n",
    "        Parámetros:\n",
    "        -----------\n",
    "        pais : str\n",
    "            Código ISO3 del país a analizar\n",
    "            \n",
    "        Retorna:\n",
    "        --------\n",
    "        dict\n",
    "            Diccionario con diferentes métricas y análisis para el país\n",
    "        \"\"\"\n",
    "        if pais not in self.resultados_dependencia:\n",
    "            raise ValueError(f\"No hay datos disponibles para {pais}\")\n",
    "            \n",
    "        resumen = {}\n",
    "        \n",
    "        # Obtener dependencia para el país\n",
    "        depend = self.resultados_dependencia[pais]\n",
    "        \n",
    "        # 1. Top 10 industrias más dependientes\n",
    "        top_10 = depend.mean(axis=1).sort_values(ascending=False).head(10)\n",
    "        resumen['top_10_industrias'] = top_10\n",
    "        \n",
    "        # 2. Estadísticas generales\n",
    "        stats = pd.DataFrame({\n",
    "            'Media': depend.mean(),\n",
    "            'Mediana': depend.median(),\n",
    "            'Max': depend.max(),\n",
    "            'Min': depend.min(),\n",
    "            'Std': depend.std()\n",
    "        })\n",
    "        resumen['estadisticas'] = stats\n",
    "        \n",
    "        # 3. Concentración por industria\n",
    "        concentracion = (depend ** 2).sum(axis=1)\n",
    "        resumen['concentracion'] = concentracion\n",
    "        \n",
    "        return resumen\n",
    "    \n",
    "\n",
    "# Preparación de datos\n",
    "result = data.copy()\n",
    "codigos_paises = list(result['exporter_iso3'].unique())\n",
    "\n",
    "# Crear instancia de la clase\n",
    "analisis = AnalisisDependenciaComercial(codigos_paises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio: 100%|██████████| 170/170 [01:11<00:00,  2.37it/s]\n"
     ]
    }
   ],
   "source": [
    "grouped_data = result.groupby('industry_descr')\n",
    "mat=analisis.crear_matriz_comercio(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mat['Mining of lignite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:\\Users\\Usuario\\Downloads\\Matriz_Mining_of_lignite.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio: 100%|██████████| 170/170 [06:12<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio iniciales...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio: 100%|██████████| 170/170 [01:13<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando dependencias para cada país...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando países: 100%|██████████| 237/237 [17:13<00:00,  4.36s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Agrupar los datos y crear matrices de comercio\n",
    "grouped_data = result.groupby('industry_descr')\n",
    "analisis.crear_matriz_comercio(grouped_data)\n",
    "# Calcular dependencias para todos los países\n",
    "resultados_dependencia = analisis.calcular_dependencias_todos_paises(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio: 100%|██████████| 170/170 [03:20<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "# Agrupar los datos y crear matrices de comercio\n",
    "grouped_data = result.groupby('industry_descr')\n",
    "matrices_de_comercio = analisis.crear_matriz_comercio(grouped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat['Mining of lignite'].to_csv(r\"C:\\Users\\Usuario\\Downloads\\matriz_comercio.csv\", sep=\";\")\n",
    "mat = mat['Mining of lignite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores_ae, matrices_normalizadas = analisis.calcular_vectores_ae('IDN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_normalizadas['Mining of lignite'].to_csv(r\"C:\\Users\\Usuario\\Downloads\\matriz_NORMALIZADA.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_O, matrices_inversas_ARE, depend= analisis.calcular_dependencia_total('IDN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices_O['Mining of lignite'].to_csv(r\"C:\\Users\\Usuario\\Downloads\\matriz_O.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "INV_ARE = matrices_inversas_ARE['Mining of lignite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert NumPy array to pandas DataFrame\n",
    "df_lignite = pd.DataFrame(matrices_inversas['Mining of lignite'])\n",
    "\n",
    "# Save to CSV\n",
    "df_lignite.to_csv(r\"C:\\Users\\Usuario\\Downloads\\matriz_inversa.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultado = np.dot( matrices_inversas['Mining of lignite'], vectores_ae['Mining of lignite'].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectores_ae['Mining of lignite'].to_csv(r\"C:\\Users\\Usuario\\Downloads\\vector_ae.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "depend.to_csv(r\"C:\\Users\\Usuario\\Downloads\\matriz_dependencia.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear un csv de datos para mostrar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesado: ABW\n",
      "Procesado: AFG\n",
      "Procesado: AGO\n",
      "Procesado: AIA\n",
      "Procesado: ALB\n",
      "Procesado: AND\n",
      "Procesado: ARE\n",
      "Procesado: ARG\n",
      "Procesado: ARM\n",
      "Procesado: ASM\n",
      "Procesado: ATA\n",
      "Procesado: ATF\n",
      "Procesado: ATG\n",
      "Procesado: AUS\n",
      "Procesado: AUT\n",
      "Procesado: AZE\n",
      "Procesado: BDI\n",
      "Procesado: BEL\n",
      "Procesado: BEN\n",
      "Procesado: BES\n",
      "Procesado: BFA\n",
      "Procesado: BGD\n",
      "Procesado: BGR\n",
      "Procesado: BHR\n",
      "Procesado: BHS\n",
      "Procesado: BIH\n",
      "Procesado: BLM\n",
      "Procesado: BLR\n",
      "Procesado: BLZ\n",
      "Procesado: BMU\n",
      "Procesado: BOL\n",
      "Procesado: BRA\n",
      "Procesado: BRB\n",
      "Procesado: BRN\n",
      "Procesado: BTN\n",
      "Procesado: BVT\n",
      "Procesado: BWA\n",
      "Procesado: CAF\n",
      "Procesado: CAN\n",
      "Procesado: CCK\n",
      "Procesado: CHE\n",
      "Procesado: CHL\n",
      "Procesado: CHN\n",
      "Procesado: CIV\n",
      "Procesado: CMR\n",
      "Procesado: COD\n",
      "Procesado: COG\n",
      "Procesado: COK\n",
      "Procesado: COL\n",
      "Procesado: COM\n",
      "Procesado: CPV\n",
      "Procesado: CRI\n",
      "Procesado: CUB\n",
      "Procesado: CUW\n",
      "Procesado: CXR\n",
      "Procesado: CYM\n",
      "Procesado: CYP\n",
      "Procesado: CZE\n",
      "Procesado: DEU\n",
      "Procesado: DJI\n",
      "Procesado: DMA\n",
      "Procesado: DNK\n",
      "Procesado: DOM\n",
      "Procesado: DZA\n",
      "Procesado: ECU\n",
      "Procesado: EGY\n",
      "Procesado: ERI\n",
      "Procesado: ESH\n",
      "Procesado: ESP\n",
      "Procesado: EST\n",
      "Procesado: ETH\n",
      "Procesado: FIN\n",
      "Procesado: FJI\n",
      "Procesado: FLK\n",
      "Procesado: FRA\n",
      "Procesado: FRE\n",
      "Procesado: FRO\n",
      "Procesado: FSM\n",
      "Procesado: GAB\n",
      "Procesado: GAZ\n",
      "Procesado: GBR\n",
      "Procesado: GEO\n",
      "Procesado: GHA\n",
      "Procesado: GIB\n",
      "Procesado: GIN\n",
      "Procesado: GMB\n",
      "Procesado: GNB\n",
      "Procesado: GNQ\n",
      "Procesado: GRC\n",
      "Procesado: GRD\n",
      "Procesado: GRL\n",
      "Procesado: GTM\n",
      "Procesado: GUM\n",
      "Procesado: GUY\n",
      "Procesado: HKG\n",
      "Procesado: HMD\n",
      "Procesado: HND\n",
      "Procesado: HRV\n",
      "Procesado: HTI\n",
      "Procesado: HUN\n",
      "Procesado: IDN\n",
      "Procesado: IND\n",
      "Procesado: IOT\n",
      "Procesado: IRL\n",
      "Procesado: IRN\n",
      "Procesado: IRQ\n",
      "Procesado: ISL\n",
      "Procesado: ISR\n",
      "Procesado: ITA\n",
      "Procesado: JAM\n",
      "Procesado: JOR\n",
      "Procesado: JPN\n",
      "Procesado: KAZ\n",
      "Procesado: KEN\n",
      "Procesado: KGZ\n",
      "Procesado: KHM\n",
      "Procesado: KIR\n",
      "Procesado: KNA\n",
      "Procesado: KOR\n",
      "Procesado: KWT\n",
      "Procesado: LAO\n",
      "Procesado: LBN\n",
      "Procesado: LBR\n",
      "Procesado: LBY\n",
      "Procesado: LCA\n",
      "Procesado: LIE\n",
      "Procesado: LKA\n",
      "Procesado: LSO\n",
      "Procesado: LTU\n",
      "Procesado: LUX\n",
      "Procesado: LVA\n",
      "Procesado: MAC\n",
      "Procesado: MAR\n",
      "Procesado: MDA\n",
      "Procesado: MDG\n",
      "Procesado: MDV\n",
      "Procesado: MEX\n",
      "Procesado: MHL\n",
      "Procesado: MKD\n",
      "Procesado: MLI\n",
      "Procesado: MLT\n",
      "Procesado: MMR\n",
      "Procesado: MNE\n",
      "Procesado: MNG\n",
      "Procesado: MNP\n",
      "Procesado: MOZ\n",
      "Procesado: MRT\n",
      "Procesado: MSR\n",
      "Procesado: MUS\n",
      "Procesado: MWI\n",
      "Procesado: MYS\n",
      "Procesado: NAM\n",
      "Procesado: NCL\n",
      "Procesado: NER\n",
      "Procesado: NFK\n",
      "Procesado: NGA\n",
      "Procesado: NIC\n",
      "Procesado: NIU\n",
      "Procesado: NLD\n",
      "Procesado: NOR\n",
      "Procesado: NPL\n",
      "Procesado: NRU\n",
      "Procesado: NZL\n",
      "Procesado: OMN\n",
      "Procesado: PAK\n",
      "Procesado: PAN\n",
      "Procesado: PCN\n",
      "Procesado: PER\n",
      "Procesado: PHL\n",
      "Procesado: PLW\n",
      "Procesado: PNG\n",
      "Procesado: POL\n",
      "Procesado: PRK\n",
      "Procesado: PRT\n",
      "Procesado: PRY\n",
      "Procesado: PSE\n",
      "Procesado: PYF\n",
      "Procesado: QAT\n",
      "Procesado: ROU\n",
      "Procesado: RUS\n",
      "Procesado: RWA\n",
      "Procesado: SAU\n",
      "Procesado: SDN\n",
      "Procesado: SEN\n",
      "Procesado: SGP\n",
      "Procesado: SGS\n",
      "Procesado: SHN\n",
      "Procesado: SLB\n",
      "Procesado: SLE\n",
      "Procesado: SLV\n",
      "Procesado: SMR\n",
      "Procesado: SOM\n",
      "Procesado: SPM\n",
      "Procesado: SRB\n",
      "Procesado: SSD\n",
      "Procesado: STP\n",
      "Procesado: SUR\n",
      "Procesado: SVK\n",
      "Procesado: SVN\n",
      "Procesado: SWE\n",
      "Procesado: SWZ\n",
      "Procesado: SXM\n",
      "Procesado: SYC\n",
      "Procesado: SYR\n",
      "Procesado: TCA\n",
      "Procesado: TCD\n",
      "Procesado: TGO\n",
      "Procesado: THA\n",
      "Procesado: TJK\n",
      "Procesado: TKL\n",
      "Procesado: TKM\n",
      "Procesado: TLS\n",
      "Procesado: TON\n",
      "Procesado: TTO\n",
      "Procesado: TUN\n",
      "Procesado: TUR\n",
      "Procesado: TUV\n",
      "Procesado: TWN\n",
      "Procesado: TZA\n",
      "Procesado: UGA\n",
      "Procesado: UKR\n",
      "Procesado: UMI\n",
      "Procesado: URY\n",
      "Procesado: USA\n",
      "Procesado: UZB\n",
      "Procesado: VAT\n",
      "Procesado: VCT\n",
      "Procesado: VEN\n",
      "Procesado: VGB\n",
      "Procesado: VNM\n",
      "Procesado: VUT\n",
      "Procesado: WLF\n",
      "Procesado: WSM\n",
      "Procesado: YEM\n",
      "Procesado: ZAF\n",
      "Procesado: ZMB\n",
      "Procesado: ZWE\n",
      "\n",
      "Proceso completado. Guardado en: c:\\Users\\Usuario\\Documents\\Github\\Seguridad económica\\src\\data\\processed\\Dependencias consolidadas\\dependencias_consolidadas.csv.gz\n",
      "Total registros: 1821444\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def transform_matrix(matrix_path):\n",
    "    \"\"\"\n",
    "    Transforma una matriz de dependencia donde:\n",
    "    - Primera columna: nombres de industrias\n",
    "    - Resto de columnas: países (códigos ISO3)\n",
    "    - Valores: dependencias\n",
    "    \"\"\"\n",
    "    # Extraer el código del país proveedor del nombre del archivo\n",
    "    supplier_country = matrix_path.split('matriz_dependencia_de_')[-1].split('.')[0]\n",
    "    \n",
    "    # Leer el CSV con la primera columna como índice\n",
    "    df = pd.read_csv(matrix_path, sep=\";\",  index_col=0)\n",
    "    \n",
    "    # Convertir a formato largo\n",
    "    df_long = df.reset_index().melt(\n",
    "        id_vars=['index'],\n",
    "        var_name='dependent_country',\n",
    "        value_name='dependency_value'\n",
    "    )\n",
    "    \n",
    "    # Renombrar la columna de industria\n",
    "    df_long = df_long.rename(columns={'index': 'industry'})\n",
    "    \n",
    "    # Añadir país proveedor y redondear valores\n",
    "    df_long['supplier_country'] = supplier_country\n",
    "    df_long['dependency_value'] = df_long['dependency_value'].round(3)\n",
    "    \n",
    "    # Filtrar valores muy pequeños y donde dependent_country no sea supplier_country\n",
    "    df_long = df_long[\n",
    "        (df_long['dependency_value'] >= 0.005) & \n",
    "        (df_long['dependent_country'] != df_long['supplier_country'])\n",
    "    ]\n",
    "    \n",
    "    # Reordenar columnas\n",
    "    return df_long[['dependent_country', 'supplier_country', 'industry', 'dependency_value']]\n",
    "\n",
    "def process_all_matrices(input_matrices: dict, clustering_file: Path):\n",
    "    \"\"\"\n",
    "    Procesa las matrices directamente de memoria y añade información de clustering\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "    errors = []\n",
    "    \n",
    "    # Procesar matrices\n",
    "    for pais, dependencia in input_matrices.items():\n",
    "        try:\n",
    "            # Obtener el DataFrame de la tupla (es el tercer elemento, índice 2)\n",
    "            matriz = dependencia[2]  # Extraemos el DataFrame de la tupla\n",
    "            \n",
    "            # Transformar la matriz a formato largo\n",
    "            df_long = matriz.reset_index().melt(\n",
    "                id_vars=['index'],\n",
    "                var_name='dependent_country',\n",
    "                value_name='dependency_value'\n",
    "            )\n",
    "            \n",
    "            # Resto del procesamiento igual\n",
    "            df_long = df_long.rename(columns={'index': 'industry'})\n",
    "            df_long['supplier_country'] = pais\n",
    "            df_long['dependency_value'] = df_long['dependency_value'].round(3)\n",
    "            \n",
    "            df_long = df_long[\n",
    "                (df_long['dependency_value'] >= 0.005) & \n",
    "                (df_long['dependent_country'] != df_long['supplier_country'])\n",
    "            ]\n",
    "            \n",
    "            df_final = df_long[['dependent_country', 'supplier_country', 'industry', 'dependency_value']]\n",
    "            all_dfs.append(df_final)\n",
    "            print(f\"Procesado: {pais}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            errors.append(f\"Error en {pais}: {str(e)}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(\"\\nErrores encontrados:\")\n",
    "        for error in errors:\n",
    "            print(error)\n",
    "    \n",
    "    if all_dfs:\n",
    "        # Combinar todas las matrices\n",
    "        combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        try:\n",
    "            # Cargar datos de clustering\n",
    "            clustering_data = pd.read_csv(clustering_file, sep=';')\n",
    "            \n",
    "            # Merge para país dependiente\n",
    "            final_df = pd.merge(\n",
    "                combined_df,\n",
    "                clustering_data[['iso_d', 'cluster']],\n",
    "                left_on='dependent_country',\n",
    "                right_on='iso_d',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Renombrar columna de cluster para país dependiente\n",
    "            final_df = final_df.rename(columns={'cluster': 'comunidad_depend'})\n",
    "            \n",
    "            # Merge para país proveedor\n",
    "            final_df = pd.merge(\n",
    "                final_df,\n",
    "                clustering_data[['iso_d', 'cluster']],\n",
    "                left_on='supplier_country',\n",
    "                right_on='iso_d',\n",
    "                how='left'\n",
    "            )\n",
    "            \n",
    "            # Renombrar columna de cluster para país proveedor\n",
    "            final_df = final_df.rename(columns={'cluster': 'comunidad_supplier'})\n",
    "            \n",
    "            # Eliminar columnas redundantes\n",
    "            final_df = final_df.drop(['iso_d_x', 'iso_d_y'], axis=1)\n",
    "            \n",
    "            # Guardar resultado final\n",
    "            # En la parte donde guardamos el archivo final, cambiar:\n",
    "            output_file = Path.cwd().parent / \"src\" / \"data\" / \"processed\" / \"Dependencias consolidadas\" / \"dependencias_consolidadas.csv\"\n",
    "            output_file_gz = output_file.with_suffix('.csv.gz')\n",
    "            output_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Guardar comprimido\n",
    "            final_df.to_csv(output_file_gz, index=False, compression='gzip')\n",
    "\n",
    "            print(f\"\\nProceso completado. Guardado en: {output_file_gz}\")\n",
    "            print(f\"Total registros: {len(final_df)}\")\n",
    "            return final_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar el archivo de clustering: {str(e)}\")\n",
    "            return combined_df\n",
    "    else:\n",
    "        raise ValueError(\"No se procesaron matrices correctamente\")\n",
    "# Uso:\n",
    "clustering_file = Path.cwd().parent / \"src\" / \"data\" / \"processed\" / \"comunidades\" / \"agglomerative_clustering_results.csv\"\n",
    "\n",
    "# Verificar que existe el archivo de clustering\n",
    "if not clustering_file.exists():\n",
    "    raise ValueError(f\"El archivo de clustering {clustering_file} no existe\")\n",
    "\n",
    "# El diccionario resultados_dependencia ya contiene todas las matrices\n",
    "combined = process_all_matrices(analisis.resultados_dependencia, clustering_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEPENDENCIA TOTAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este fragmento de código, se lleva a cabo un análisis de dependencia económica entre diferentes industrias utilizando datos de comercio internacional. \n",
    "\n",
    "En esta fase se calcula lo que sería la dependencia sin separar los países \"terceros\".\n",
    "\n",
    "Inicialmente, se crea una lista de códigos de países que incluye \"tercero\". Luego, los datos se agrupan por descripción de la industria y se procesan para crear matrices de comercio internacional, las cuales representan las relaciones comerciales entre países en cada industria. Posteriormente, se normalizan estas matrices y se calcula un vector específico para cada industria. Además, se genera una matriz de dependencia económica utilizando la inversa de la matriz normalizada. Finalmente, se renombran las columnas de esta matriz con los códigos de países de la Unión Europea.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creando matrices de comercio:  36%|███▌      | 61/170 [00:59<01:45,  1.03it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m grouped_data \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindustry_descr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#Obtengo matrices de flujos comerciales por industria y país\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m matrices \u001b[38;5;241m=\u001b[39m \u001b[43mcrear_matriz_comercio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouped_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodigos_paises\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Diccionarios para almacenar resultados\u001b[39;00m\n\u001b[0;32m     20\u001b[0m resultados_dependencia \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m, in \u001b[0;36mcrear_matriz_comercio\u001b[1;34m(grouped_data, codigos_paises)\u001b[0m\n\u001b[0;32m      5\u001b[0m importers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(codigos_paises)\n\u001b[0;32m      6\u001b[0m matrix \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(exporters), \u001b[38;5;28mlen\u001b[39m(importers))), index\u001b[38;5;241m=\u001b[39mexporters, columns\u001b[38;5;241m=\u001b[39mimporters)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m group\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m      9\u001b[0m     matrix\u001b[38;5;241m.\u001b[39mloc[row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexporter_iso3\u001b[39m\u001b[38;5;124m'\u001b[39m], row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimporter_iso3\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrade\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     11\u001b[0m matrix\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;241m0\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\envs\\tftimeseries\\lib\\site-packages\\pandas\\core\\frame.py:1263\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1261\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced\n\u001b[0;32m   1262\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m-> 1263\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m k, s\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\envs\\tftimeseries\\lib\\site-packages\\pandas\\core\\series.py:443\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    441\u001b[0m manager \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 443\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mSingleBlockManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    445\u001b[0m     data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\anaconda3\\envs\\tftimeseries\\lib\\site-packages\\pandas\\core\\internals\\managers.py:1574\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[1;34m(cls, array, index)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_array\u001b[39m(\u001b[38;5;28mcls\u001b[39m, array: ArrayLike, index: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SingleBlockManager:\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1572\u001b[0m \u001b[38;5;124;03m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[0;32m   1573\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1574\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mnew_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(block, index)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Copiar la base de datos original\n",
    "result = itp2019.copy()\n",
    "\n",
    "# Crear una lista con todos los códigos de países\n",
    "unique = result['exporter_iso3'].unique()\n",
    "codigos_paises = list(unique)\n",
    "\n",
    "\n",
    "# Primero, agrupamos los datos por 'industry_descr'\n",
    "grouped_data = result.groupby('industry_descr')\n",
    "\n",
    "#Obtengo matrices de flujos comerciales por industria y país\n",
    "\n",
    "matrices = crear_matriz_comercio(grouped_data, codigos_paises)\n",
    "\n",
    "\n",
    "\n",
    "# Diccionarios para almacenar resultados\n",
    "\n",
    "resultados_dependencia = {}\n",
    "\n",
    "# Iterar sobre la lista de países con una barra de progreso\n",
    "for pais in tqdm(codigos_paises, desc=\"Procesando países\"):\n",
    "    # Obtener vectores ae y matrices normalizadas\n",
    "    vectores_ae, matrices_normalizadas = calcular_vectores_ae(matrices, pais)\n",
    "    \n",
    "    # Construir matrices Omega\n",
    "    matrices_O = obtener_submatrices(matrices_normalizadas)\n",
    "    \n",
    "    # Calcular las matrices inversas\n",
    "    matrices_inversas = calcular_matrices_inversas(matrices_O)\n",
    "    \n",
    "    # Calcular dependencia\n",
    "    depend = calcular_dependencia(vectores_ae, matrices_inversas)\n",
    "    \n",
    "    # Guardar los resultados en los diccionarios\n",
    "    resultados_dependencia[pais] = depend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando países: 100%|██████████| 237/237 [27:58<00:00,  7.08s/it]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "resultados_dependencia = {}\n",
    "\n",
    "# Iterar sobre la lista de países con una barra de progreso\n",
    "for pais in tqdm(codigos_paises, desc=\"Procesando países\"):\n",
    "    # Obtener vectores ae y matrices normalizadas\n",
    "    vectores_ae, matrices_normalizadas = calcular_vectores_ae(matrices, pais)\n",
    "    \n",
    "    # Construir matrices Omega\n",
    "    matrices_O = obtener_submatrices(matrices_normalizadas)\n",
    "    \n",
    "    # Calcular las matrices inversas\n",
    "    matrices_inversas = calcular_matrices_inversas(matrices_O)\n",
    "    \n",
    "    # Calcular dependencia\n",
    "    depend = calcular_dependencia(vectores_ae, matrices_inversas)\n",
    "    \n",
    "    # Guardar los resultados en los diccionarios\n",
    "    resultados_dependencia[pais] = depend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyo dataframes con estructura de panel para dependencias y pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando países: 100%|██████████| 237/237 [00:02<00:00, 114.51it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Crear un diccionario para almacenar los DataFrames\n",
    "dict_of_dfs = {}\n",
    "\n",
    "# Agrupar por 'exporter_iso3'\n",
    "grouped = result.groupby('exporter_iso3')\n",
    "\n",
    "# Iterar sobre cada grupo\n",
    "for exporter, group in grouped:\n",
    "    # Pivotar el DataFrame\n",
    "    pivot_df = group.pivot(index='industry_descr', columns='importer_iso3', values='trade')\n",
    "\n",
    "    # Reemplazar NaN con ceros\n",
    "    pivot_df = pivot_df.fillna(0)\n",
    "    \n",
    "    # Sumar las columnas\n",
    "    col_sums = pivot_df.sum(axis=0)\n",
    "    \n",
    "    # Dividir cada celda por el total de su columna\n",
    "    normalized_df = pivot_df.div(col_sums, axis=1)\n",
    "    \n",
    "    # Eliminar la columna que se llama igual que el valor de exporter_iso3\n",
    "    if exporter in normalized_df.columns:\n",
    "        normalized_df = normalized_df.drop(columns=[exporter])\n",
    "    \n",
    "    # Almacenar el DataFrame normalizado en el diccionario\n",
    "    dict_of_dfs[exporter] = normalized_df\n",
    "    \n",
    "reshape_dfs = {}\n",
    "for country, df in tqdm(dict_of_dfs.items(), desc=\"Procesando países\"):\n",
    "    # Resetear el índice y moverlo a una nueva columna llamada 'index_column'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'industry_descr'}, inplace=True)\n",
    "    # Reshape del DataFrame usando melt\n",
    "    reshaped_df = df.melt(id_vars='industry_descr', var_name='importer', value_name='value')\n",
    "    reshaped_df['exporter'] =country\n",
    "    reshape_dfs[country] = reshaped_df\n",
    "\n",
    "concatenated_w = pd.concat(reshape_dfs.values(), ignore_index=True).fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo un dataframe donde pongo columnas exportadores e importadores, industrias y pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando países: 100%|██████████| 237/237 [00:02<00:00, 91.28it/s] \n"
     ]
    }
   ],
   "source": [
    "   \n",
    "reshape_dfs = {}\n",
    "for country, df in tqdm(resultados_dependencia.items(), desc=\"Procesando países\"):\n",
    "    # Resetear el índice y moverlo a una nueva columna llamada 'index_column'\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'industry_descr'}, inplace=True)\n",
    "    # Reshape del DataFrame usando melt\n",
    "    reshaped_df = df.melt(id_vars='industry_descr', var_name='importer', value_name='dep')\n",
    "    reshaped_df['exporter'] =country\n",
    "    reshape_dfs[country] = reshaped_df\n",
    "\n",
    "concatenated_dep = pd.concat(reshape_dfs.values(), ignore_index=True).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(concatenated_w, concatenated_dep, on=['industry_descr', 'importer', 'exporter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['dep_peso'] = merged_df['value']*merged_df['dep']\n",
    "\n",
    "# Seleccionar las columnas relevantes\n",
    "merged_df = merged_df[['importer', 'exporter', 'dep_peso']]\n",
    "\n",
    "# Agrupar por \"exporter\" e \"importer\" y sumar los valores de \"dep_peso\"\n",
    "merged_df_peso = merged_df.groupby(['exporter', 'importer']).sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_peso.to_excel(r\"C:\\Users\\Usuario\\Downloads\\import_dep.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tftimeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
